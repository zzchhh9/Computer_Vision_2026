{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XRxHiKdGHiT"
   },
   "source": [
    "# Coursework 2: Image segmentation\n",
    "\n",
    "In this coursework you will develop and train a convolutional neural network for brain tumour segmentation. Please read both the text and the code in this notebook to get an idea what you are expected to implement. Pay attention to the missing code blocks that look like this:\n",
    "\n",
    "```\n",
    "### Insert your code ###\n",
    "...\n",
    "### End of your code ###\n",
    "```\n",
    "## What is expected?\n",
    "\n",
    "* Complete and run the code using `jupyter-lab`.\n",
    "\n",
    "* Export (File | Save and Export Notebook As...) the notebook as a PDF file, which contains your code, results and answers, and upload the PDF file onto [Scientia](https://scientia.doc.ic.ac.uk).\n",
    "\n",
    "* If Jupyter complains issues during exporting, it is likely that [pandoc](https://pandoc.org/installing.html) or latex is not installed, or their paths have not been included. You can install the relevant libraries and retry. Alternatively, use the Print function of your browser to export the PDF file.\n",
    "\n",
    "* If Jupyter-lab does not work for you at the end, alternatively, you can use Google Colab to write the code and export the PDF file.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "You need to install Jupyter-Lab (https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html) and other libraries used in this coursework, such as by running the command:\n",
    "`pip3 install [package_name]`\n",
    "\n",
    "## GPU resource\n",
    "\n",
    "The coursework is developed to be able to run on CPU, as all images have been pre-processed to be 2D and of a smaller size, compared to original 3D volumes.\n",
    "\n",
    "However, to save training time, you may want to use GPU. In that case, you can run this notebook on Google Colab. On Google Colab, go to the menu, Runtime - Change runtime type, and select **GPU** as the hardware acceleartor. At the end, please still export everything and submit as a PDF file on Scientia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eq1KWmR3HWYV"
   },
   "outputs": [],
   "source": "# Import libraries\n# These libraries should be sufficient for this tutorial.\n# However, if any other library is needed, please install by yourself.\nimport tarfile\nimport imageio\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport time\nimport os\nimport random\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4TX-CXBHW4c"
   },
   "source": [
    "## Q1. Download and visualise the imaging dataset.\n",
    "\n",
    "The dataset is a public brain imaging dataset from [Medical Decathlon Challenge](http://medicaldecathlon.com/). To save the storage and reduce the computational cost for this tutorial, we extract 2D image slices from the original 3D volumes (T1-Gd contrast enhanced imaging) and downsample the 2D images.\n",
    "\n",
    "The dataset consists of a training set and a test set. Each image is of dimension 120 x 120, with a corresponding label map of the same dimension. There are four number of classes in the label map:\n",
    "\n",
    "- 0: background\n",
    "- 1: edema\n",
    "- 2: non-enhancing tumour\n",
    "- 3: enhancing tumour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mt93oQ8xZkE9"
   },
   "outputs": [],
   "source": [
    "# Download the dataset\n",
    "# If you use Ubuntu, wget would natively work.\n",
    "# If you use Mac or Windows, which does not have the wget command, you can copy the URL to the web browser and download the file.\n",
    "!wget https://www.dropbox.com/s/zmytk2yu284af6t/Task01_BrainTumour_2D.tar.gz\n",
    "\n",
    "# Unzip the '.tar.gz' file to the current directory\n",
    "datafile = tarfile.open('Task01_BrainTumour_2D.tar.gz')\n",
    "datafile.extractall()\n",
    "datafile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu_BTL0x6o5a"
   },
   "source": [
    "## Visualise a random set of 4 training images along with their label maps.\n",
    "\n",
    "Suggested colour map for brain MR image:\n",
    "```\n",
    "cmap = 'gray'\n",
    "```\n",
    "\n",
    "Suggested colour map for segmentation map:\n",
    "```\n",
    "cmap = colors.ListedColormap(['black', 'green', 'blue', 'red'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fgubCRC6m4k"
   },
   "outputs": [],
   "source": "### Insert your code ###\n# Visualize 4 random training images with their label maps\ntrain_image_path = 'Task01_BrainTumour_2D/training_images'\ntrain_label_path = 'Task01_BrainTumour_2D/training_labels'\n\n# Get list of image names\nimage_names = sorted(os.listdir(train_image_path))\n\n# Randomly select 4 images\nrandom_indices = random.sample(range(len(image_names)), 4)\n\n# Create figure with 4 rows and 2 columns\nfig, axes = plt.subplots(4, 2, figsize=(8, 16))\n\n# Define colormaps\nimg_cmap = 'gray'\nseg_cmap = colors.ListedColormap(['black', 'green', 'blue', 'red'])\n\nfor i, idx in enumerate(random_indices):\n    image_name = image_names[idx]\n    \n    # Read image and label\n    image = imageio.v2.imread(os.path.join(train_image_path, image_name))\n    label = imageio.v2.imread(os.path.join(train_label_path, image_name))\n    \n    # Display image\n    axes[i, 0].imshow(image, cmap=img_cmap)\n    axes[i, 0].set_title(f'Image: {image_name}')\n    axes[i, 0].axis('off')\n    \n    # Display label map\n    axes[i, 1].imshow(label, cmap=seg_cmap, vmin=0, vmax=3)\n    axes[i, 1].set_title(f'Label map')\n    axes[i, 1].axis('off')\n\nplt.tight_layout()\nplt.show()\n### End of your code ###"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5xWGT3KaML-D"
   },
   "source": [
    "## Q2. Implement a dataset class.\n",
    "\n",
    "It can read the imaging dataset and get items, pairs of images and label maps, to be used as training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6p6wFZ3na5z9"
   },
   "outputs": [],
   "source": "def normalise_intensity(image, thres_roi=1.0):\n    \"\"\" Normalise the image intensity by the mean and standard deviation \"\"\"\n    # ROI defines the image foreground\n    val_l = np.percentile(image, thres_roi)\n    roi = (image >= val_l)\n    mu, sigma = np.mean(image[roi]), np.std(image[roi])\n    eps = 1e-6\n    image2 = (image - mu) / (sigma + eps)\n    return image2\n\n\nclass BrainImageSet(Dataset):\n    \"\"\" Brain image set \"\"\"\n    def __init__(self, image_path, label_path='', deploy=False):\n        self.image_path = image_path\n        self.deploy = deploy\n        self.images = []\n        self.labels = []\n\n        image_names = sorted(os.listdir(image_path))\n        for image_name in image_names:\n            # Read the image\n            image = imageio.v2.imread(os.path.join(image_path, image_name))\n            self.images += [image]\n\n            # Read the label map\n            if not self.deploy:\n                label_name = os.path.join(label_path, image_name)\n                label = imageio.v2.imread(label_name)\n                self.labels += [label]\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        # Get an image and perform intensity normalisation\n        # Dimension: XY\n        image = normalise_intensity(self.images[idx])\n\n        # Get its label map\n        # Dimension: XY\n        label = self.labels[idx]\n        return image, label\n\n    def get_random_batch(self, batch_size):\n        # Get a batch of paired images and label maps\n        # Dimension of images: NCXY\n        # Dimension of labels: NXY\n        images, labels = [], []\n\n        ### Insert your code ###\n        # Randomly sample batch_size indices\n        indices = random.sample(range(len(self)), batch_size)\n        \n        for idx in indices:\n            image, label = self.__getitem__(idx)\n            # Add channel dimension to image (from XY to CXY where C=1)\n            image = image[np.newaxis, :, :]\n            images.append(image)\n            labels.append(label)\n        \n        # Stack to create batch dimension: NCXY for images, NXY for labels\n        images = np.stack(images, axis=0)\n        labels = np.stack(labels, axis=0)\n        ### End of your code ###\n        return images, labels"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pa4ZpawDNmwu"
   },
   "source": [
    "## Q3. Build a U-net architecture.\n",
    "\n",
    "Implement a U-net architecture for image segmentation. If you are not familiar with U-net, you can read this paper:\n",
    "\n",
    "[1] Olaf Ronneberger et al. [U-Net: Convolutional networks for biomedical image segmentation](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28). MICCAI, 2015.\n",
    "\n",
    "For the first convolutional layer, you can start with 16 filters. We have implemented the encoder path. Please complete the decoder path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMPmBZVGb1aI"
   },
   "outputs": [],
   "source": "\"\"\" U-net \"\"\"\nclass UNet(nn.Module):\n    def __init__(self, input_channel=1, output_channel=1, num_filter=16):\n        super(UNet, self).__init__()\n\n        # BatchNorm: by default during training this layer keeps running estimates\n        # of its computed mean and variance, which are then used for normalization\n        # during evaluation.\n\n        # Encoder path\n        n = num_filter  # 16\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(input_channel, n, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n),\n            nn.ReLU(),\n            nn.Conv2d(n, n, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n),\n            nn.ReLU()\n        )\n\n        n *= 2  # 32\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(n),\n            nn.ReLU(),\n            nn.Conv2d(n, n, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n),\n            nn.ReLU()\n        )\n\n        n *= 2  # 64\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(n),\n            nn.ReLU(),\n            nn.Conv2d(n, n, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n),\n            nn.ReLU()\n        )\n\n        n *= 2  # 128\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(int(n / 2), n, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(n),\n            nn.ReLU(),\n            nn.Conv2d(n, n, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n),\n            nn.ReLU()\n        )\n        \n        # Decoder path\n        ### Insert your code ###\n        # n = 128 at this point\n        # Upsample from 128 -> 64, then concat with skip (64) -> 128, then conv to 64\n        self.upconv3 = nn.Sequential(\n            nn.ConvTranspose2d(n, int(n / 2), kernel_size=2, stride=2),\n            nn.BatchNorm2d(int(n / 2)),\n            nn.ReLU()\n        )\n        n = int(n / 2)  # 64\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(n * 2, n, kernel_size=3, padding=1),  # 128 -> 64 (after concat)\n            nn.BatchNorm2d(n),\n            nn.ReLU(),\n            nn.Conv2d(n, n, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n),\n            nn.ReLU()\n        )\n        \n        # Upsample from 64 -> 32, then concat with skip (32) -> 64, then conv to 32\n        self.upconv2 = nn.Sequential(\n            nn.ConvTranspose2d(n, int(n / 2), kernel_size=2, stride=2),\n            nn.BatchNorm2d(int(n / 2)),\n            nn.ReLU()\n        )\n        n = int(n / 2)  # 32\n        self.conv6 = nn.Sequential(\n            nn.Conv2d(n * 2, n, kernel_size=3, padding=1),  # 64 -> 32 (after concat)\n            nn.BatchNorm2d(n),\n            nn.ReLU(),\n            nn.Conv2d(n, n, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n),\n            nn.ReLU()\n        )\n        \n        # Upsample from 32 -> 16, then concat with skip (16) -> 32, then conv to 16\n        self.upconv1 = nn.Sequential(\n            nn.ConvTranspose2d(n, int(n / 2), kernel_size=2, stride=2),\n            nn.BatchNorm2d(int(n / 2)),\n            nn.ReLU()\n        )\n        n = int(n / 2)  # 16\n        self.conv7 = nn.Sequential(\n            nn.Conv2d(n * 2, n, kernel_size=3, padding=1),  # 32 -> 16 (after concat)\n            nn.BatchNorm2d(n),\n            nn.ReLU(),\n            nn.Conv2d(n, n, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n),\n            nn.ReLU()\n        )\n        \n        # Final 1x1 convolution to get the output segmentation\n        self.final_conv = nn.Conv2d(n, output_channel, kernel_size=1)\n        ### End of your code ###\n\n    def forward(self, x):\n        # Use the convolutional operators defined above to build the U-net\n        # The encoder part is already done for you.\n        # You need to complete the decoder part.\n        # Encoder\n        x = self.conv1(x)\n        conv1_skip = x\n\n        x = self.conv2(x)\n        conv2_skip = x\n\n        x = self.conv3(x)\n        conv3_skip = x\n\n        x = self.conv4(x)\n\n        # Decoder\n        ### Insert your code ###\n        # Upsample and concatenate with conv3_skip\n        x = self.upconv3(x)\n        x = torch.cat([x, conv3_skip], dim=1)\n        x = self.conv5(x)\n        \n        # Upsample and concatenate with conv2_skip\n        x = self.upconv2(x)\n        x = torch.cat([x, conv2_skip], dim=1)\n        x = self.conv6(x)\n        \n        # Upsample and concatenate with conv1_skip\n        x = self.upconv1(x)\n        x = torch.cat([x, conv1_skip], dim=1)\n        x = self.conv7(x)\n        \n        # Final convolution to get output segmentation\n        x = self.final_conv(x)\n        ### End of your code ###\n        return x"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcNWZS08d47P"
   },
   "source": [
    "## Q4. Train the segmentation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaGGkKQndIaR"
   },
   "outputs": [],
   "source": "# CUDA device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Device: {0}'.format(device))\n\n# Build the model\nnum_class = 4\nmodel = UNet(input_channel=1, output_channel=num_class, num_filter=16)\nmodel = model.to(device)\nparams = list(model.parameters())\n\nmodel_dir = 'saved_models'\nif not os.path.exists(model_dir):\n    os.makedirs(model_dir)\n\n# Optimizer\noptimizer = optim.Adam(params, lr=1e-3)\n\n# Segmentation loss\ncriterion = nn.CrossEntropyLoss()\n\n# Datasets\ntrain_set = BrainImageSet('Task01_BrainTumour_2D/training_images', 'Task01_BrainTumour_2D/training_labels')\ntest_set = BrainImageSet('Task01_BrainTumour_2D/test_images', 'Task01_BrainTumour_2D/test_labels')\n\n# Train the model\n# Note: when you debug the model, you may reduce the number of iterations or batch size to save time.\nnum_iter = 10000\ntrain_batch_size = 16\neval_batch_size = 16\nstart = time.time()\nfor it in range(1, 1 + num_iter):\n    # Set the modules in training mode, which will have effects on certain modules, e.g. dropout or batchnorm.\n    start_iter = time.time()\n    model.train()\n\n    # Get a batch of images and labels\n    images, labels = train_set.get_random_batch(train_batch_size)\n    images, labels = torch.from_numpy(images), torch.from_numpy(labels)\n    images, labels = images.to(device, dtype=torch.float32), labels.to(device, dtype=torch.long)\n    logits = model(images)\n\n    # Perform optimisation and print out the training loss\n    ### Insert your code ###\n    # Compute the loss\n    loss = criterion(logits, labels)\n    \n    # Zero gradients, backward pass, and update weights\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    # Print training loss every 100 iterations\n    if it % 100 == 0:\n        print(f'Iteration {it}/{num_iter}, Training Loss: {loss.item():.4f}, Time: {time.time() - start_iter:.3f}s')\n    ### End of your code ###\n\n    # Evaluate\n    if it % 1000 == 0:\n        model.eval()\n        # Disabling gradient calculation during reference to reduce memory consumption\n        with torch.no_grad():\n            # Evaluate on a batch of test images and print out the test loss\n            ### Insert your code ###\n            test_images, test_labels = test_set.get_random_batch(eval_batch_size)\n            test_images, test_labels = torch.from_numpy(test_images), torch.from_numpy(test_labels)\n            test_images, test_labels = test_images.to(device, dtype=torch.float32), test_labels.to(device, dtype=torch.long)\n            test_logits = model(test_images)\n            test_loss = criterion(test_logits, test_labels)\n            \n            # Calculate accuracy\n            test_pred = torch.argmax(test_logits, dim=1)\n            test_acc = (test_pred == test_labels).float().mean()\n            \n            print(f'Iteration {it}/{num_iter}, Test Loss: {test_loss.item():.4f}, Test Accuracy: {test_acc.item():.4f}')\n            ### End of your code ###\n\n    # Save the model\n    if it % 5000 == 0:\n        torch.save(model.state_dict(), os.path.join(model_dir, 'model_{0}.pt'.format(it)))\nprint('Training took {:.3f}s in total.'.format(time.time() - start))"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "89yjxjGyb6yT"
   },
   "source": [
    "## Q5. Deploy the trained model to a random set of 4 test images and visualise the automated segmentation.\n",
    "\n",
    "You can show the images as a 4 x 3 panel. Each row shows one example, with the 3 columns being the test image, automated segmentation and ground truth segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZeLE0qZjd2j"
   },
   "outputs": [],
   "source": "### Insert your code ###\n# Deploy the trained model to 4 random test images and visualize results\n\n# Set model to evaluation mode\nmodel.eval()\n\n# Get 4 random test images\ntest_images, test_labels = test_set.get_random_batch(4)\ntest_images_tensor = torch.from_numpy(test_images).to(device, dtype=torch.float32)\n\n# Run inference\nwith torch.no_grad():\n    predictions = model(test_images_tensor)\n    predictions = torch.argmax(predictions, dim=1).cpu().numpy()\n\n# Create visualization: 4 rows x 3 columns (image, prediction, ground truth)\nfig, axes = plt.subplots(4, 3, figsize=(12, 16))\n\n# Define colormaps\nimg_cmap = 'gray'\nseg_cmap = colors.ListedColormap(['black', 'green', 'blue', 'red'])\n\nfor i in range(4):\n    # Display the test image (remove channel dimension)\n    axes[i, 0].imshow(test_images[i, 0], cmap=img_cmap)\n    axes[i, 0].set_title('Test Image')\n    axes[i, 0].axis('off')\n    \n    # Display automated segmentation (prediction)\n    axes[i, 1].imshow(predictions[i], cmap=seg_cmap, vmin=0, vmax=3)\n    axes[i, 1].set_title('Automated Segmentation')\n    axes[i, 1].axis('off')\n    \n    # Display ground truth segmentation\n    axes[i, 2].imshow(test_labels[i], cmap=seg_cmap, vmin=0, vmax=3)\n    axes[i, 2].set_title('Ground Truth')\n    axes[i, 2].axis('off')\n\nplt.tight_layout()\nplt.show()\n\n# Calculate and print Dice score for each class\ndef dice_score(pred, gt, class_idx):\n    pred_class = (pred == class_idx)\n    gt_class = (gt == class_idx)\n    intersection = np.sum(pred_class & gt_class)\n    union = np.sum(pred_class) + np.sum(gt_class)\n    if union == 0:\n        return 1.0 if intersection == 0 else 0.0\n    return 2 * intersection / union\n\nclass_names = ['Background', 'Edema', 'Non-enhancing tumour', 'Enhancing tumour']\nprint('\\nDice scores for the 4 test images:')\nfor class_idx in range(4):\n    dice_scores = [dice_score(predictions[i], test_labels[i], class_idx) for i in range(4)]\n    avg_dice = np.mean(dice_scores)\n    print(f'{class_names[class_idx]}: {avg_dice:.4f}')\n### End of your code ###"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cj3Qusin_s_r"
   },
   "source": [
    "## Q6. Discussion. Does your trained model work well? How would you improve this model so it can be deployed to the real clinic?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "nVwEtDKIdTRs"
   },
   "outputs": [],
   "source": "## Discussion\n\n**Does the trained model work well?**\n\nThe model demonstrates reasonable performance for brain tumour segmentation, successfully identifying tumour regions in most test images. However, there are limitations:\n- The model may struggle with small tumour regions due to class imbalance (background dominates)\n- Boundary segmentation may not be precise, showing some over/under-segmentation\n- Performance varies across different tumour types (edema, non-enhancing, enhancing)\n\n**How would you improve this model for real clinic deployment?**\n\n1. **Data improvements:**\n   - Use larger training datasets with more diverse patient populations\n   - Apply data augmentation (rotation, flipping, elastic deformation, intensity variations)\n   - Use 3D volumes instead of 2D slices to capture spatial context\n   - Include multi-modal MRI (T1, T2, FLAIR, T1-Gd) as multiple input channels\n\n2. **Architecture improvements:**\n   - Use deeper networks or attention mechanisms (e.g., Attention U-Net)\n   - Implement residual connections for better gradient flow\n   - Consider 3D U-Net for volumetric segmentation\n   - Use ensemble methods combining multiple models\n\n3. **Training improvements:**\n   - Address class imbalance using Dice loss or focal loss instead of cross-entropy\n   - Use learning rate scheduling and early stopping\n   - Apply more extensive hyperparameter tuning\n   - Implement cross-validation for robust evaluation\n\n4. **Clinical deployment considerations:**\n   - Rigorous validation on external datasets from different scanners/institutions\n   - Uncertainty quantification to flag low-confidence predictions\n   - Integration with clinical workflow (PACS systems)\n   - Regulatory approval (FDA/CE marking)\n   - Human-in-the-loop verification by radiologists\n   - Continuous monitoring and model updates as new data becomes available"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}